Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

%%%% a few items added manually %%%%%
@misc{Tantau2022,
	title={User’s Guide to the Beamer Class, Version 3.66},
	author={Tantau, Till},
	year={2022}
}

@inproceedings{Liu2020,
	author = {Liu, Siyou and Zhang, Xiaojun},
	booktitle = {Proceedings of the 12th Conference on Language Resources and Evaluation},
	pages = {3775--3781},
	title = {{Corpora for Document-Level Neural Machine Translation}},
	url = {http://opus.nlpl.eu/OpenSubtitles2018.},
	year = {2020}
}

@article{DBLP:journals/corr/abs-1912-09723,
	publtype={informal},
	author={Pavel Efimov and Leonid Boytsov and Pavel Braslavski},
	title={SberQuAD - Russian Reading Comprehension Dataset: Description and Analysis},
	year={2019},
	cdate={1546300800000},
	journal={CoRR},
	volume={abs/1912.09723},
	url={http://arxiv.org/abs/1912.09723}
}

%%%%%%%%%%%

@inproceedings{Scarton2018,
abstract = {Text simplification (TS) is a monolingual text-to-text transformation task where an original (complex) text is transformed into a target (simpler) text. Most recent work is based on sequence-to-sequence neural models similar to those used for machine translation (MT). Different from MT, TS data comprises more elaborate transformations , such as sentence splitting. It can also contain multiple simplifications of the same original text targeting different audiences , such as school grade levels. We explore these two features of TS to build models tailored for specific grade levels. Our approach uses a standard sequence-to-sequence architecture where the original sequence is annotated with information about the target audience and/or the (predicted) type of simplification operation. We show that it outperforms state-of-the-art TS approaches (up to 3 and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning.},
address = {Melbourne},
author = {Scarton, Carolina and Specia, Lucia},
booktitle = {Proceedings ofthe 56th Annual Meeting ofthe Association for Computational Linguistics (Short Papers)},
file = {:home/maria/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scarton, Specia - Unknown - Learning Simplifications for Specific Target Audiences.pdf:pdf},
month = {jul},
pages = {712--718},
publisher = {Association for Computational Linguistics},
title = {{Learning Simplifications for Specific Target Audiences}},
url = {http://opennmt.net/OpenNMT/},
year = {2018}
}
@inproceedings{Battisti2020,
address = {Marseille},
archivePrefix = {arXiv},
arxivId = {1909.09067v1},
author = {Battisti, Alessia and Pf{\"{u}}tze, Dominik and Pf{\"{u}}tze, Pf¨ and S{\"{a}}uberli, Andreas and Kostrzewa, Marek and Ebling, Sarah},
booktitle = {Proceedings ofthe 12th Conference on Language Resources and Evaluation},
eprint = {1909.09067v1},
file = {:home/maria/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Battisti et al. - 2020 - A Corpus for Automatic Readability Assessment and Text Simplification of German.pdf:pdf},
keywords = {Simplified German,automatic readability assessment,automatic text simplification,multimodal},
pages = {11--16},
publisher = {European Language Resources Association (ELRA)},
title = {{A Corpus for Automatic Readability Assessment and Text Simplification of German}},
url = {https://www.pdflib.com/},
year = {2020}
}
@inproceedings{Alva-Manchego2017,
abstract = {Current research in text simplification has been hampered by two central problems: (i) the small amount of high-quality parallel simplification data available, and (ii) the lack of explicit annotations of simplification operations, such as deletions or substitutions , on existing data. While the recently introduced Newsela corpus has alleviated the first problem, simplifications still need to be learned directly from parallel text using black-box, end-to-end approaches rather than from explicit annotations. These complex-simple parallel sentence pairs often differ to such a high degree that generalization becomes difficult. End-to-end models also make it hard to interpret what is actually learned from data. We propose a method that decomposes the task of TS into its sub-problems. We devise a way to automatically identify operations in a parallel corpus and introduce a sequence-labeling approach based on these annotations. Finally, we provide insights on the types of transformations that different approaches can model.},
address = {Taipei, Taiwan},
author = {Alva-Manchego, Fernando and Bingel, Joachim and Paetzold, Gustavo H and Scarton, Carolina and Specia, Lucia},
booktitle = {Proceedings ofthe The 8th International Joint Conference on Natural Language Processing},
file = {:home/maria/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alva-Manchego et al. - Unknown - Learning How to Simplify From Explicit Labeling of Complex-Simplified Text Pairs.pdf:pdf},
pages = {295--305},
publisher = {AFNLP},
title = {{Learning How to Simplify From Explicit Labeling of Complex-Simplified Text Pairs}},
url = {https://github.com/jbingel/},
year = {2017}
}
@inproceedings{Alva-Manchego2020,
	abstract = {In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences , paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.},
	author = {Alva-Manchego, Fernando and Martin, Louis and Bordes, Antoine and Scarton, Carolina and Sagot, Benoˆıt Benoˆıt and Specia, Lucia},
	booktitle = {Proceedings ofthe 58th Annual Meeting ofthe Association for Computational Linguistics},
	file = {:home/maria/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alva-Manchego et al. - Unknown - ASSET A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Tra.pdf:pdf},
	mendeley-groups = {7{\_}NLP/tasks/complexity},
	month = {jul},
	pages = {4668--4679},
	publisher = {Association for Computational Linguistics},
	title = {{ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations}},
	url = {https://github.com/facebookresearch/},
	year = {2020}
}


@inproceedings{Agrawal2019,
abstract = {This work introduces a machine translation task where the output is aimed at audiences of different levels of target language proficiency. We collect a high quality dataset of news articles available in English and Spanish, written for diverse grade levels and propose a method to align segments across comparable bilingual articles. The resulting dataset makes it possible to train multi-task sequence-to-sequence models that translate Spanish into English targeted at an easier reading grade level than the original Spanish. We show that these multi-task models outperform pipeline approaches that translate and simplify text independently.},
address = {Hong Kong, China},
archivePrefix = {arXiv},
arxivId = {1911.00835v1},
author = {Agrawal, Sweta and Carpuat, Marine},
booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing},
eprint = {1911.00835v1},
file = {:home/maria/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Agrawal, Carpuat - Unknown - Controlling Text Complexity in Neural Machine Translation(2).pdf:pdf},
pages = {1549--1564},
publisher = {Association for Computational Linguistics},
title = {{Controlling Text Complexity in Neural Machine Translation}},
url = {https://newsela.com/data/.},
year = {2019}
}
@inproceedings{Tani2022,
author = {Tani, Kazuki and Yuasa, Ryoya and Takikawa, Kauzki and Tamura, Akihiro and Kajiwara, Tomoyuki and Ninomiya, Takashi and Kato, Tsuneo},
booktitle = {LREC 2022 submission},
file = {:home/maria/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tani et al. - 2019 - A Benchmark Dataset for Multi-Level Complexity-Controllable Machine Translation.pdf:pdf},
keywords = {annotation,corpus,creation,etc,machine translation,natural language generation},
title = {{A Benchmark Dataset for Multi-Level Complexity-Controllable Machine Translation}},
year = {2022}
}
@article{Alva-Manchego2021,
abstract = {In order to simplify sentences, several rewriting operations can be performed, such as replacing complex words per simpler synonyms, deleting unnecessary information, and splitting long sentences. Despite this multi-operation nature, evaluation of automatic simplification systems relies on metrics that moderately correlate with human judgments on the simplicity achieved by executing specific operations (e.g., simplicity gain based on lexical replacements). In this article, we investigate how well existing metrics can assess sentence-level simplifications where multiple operations may have been applied and which, therefore, require more general simplicity judgments. For that, we first collect a new and more reliable data set for evaluating the correlation of metrics and human judgments of overall simplicity. Second, we conduct the first meta-evaluation of automatic metrics in Text Simplification, using our new data set (and other existing data) to analyze the variation of the correlation between metrics' scores and human judgments across three dimensions: the perceived simplicity level, the system type, and the set of references used for computation. We show that these three aspects affect the correlations and, in particular, highlight the limitations of commonly used operation-specific metrics. Finally, based on our findings, we propose a set of recommendations for automatic evaluation of multi-operation simplifications, suggesting which metrics to compute and how to interpret their scores.},
author = {Alva-Manchego, Fernando and Scarton, Carolina and Specia, Lucia},
doi = {10.1162/COLI},
file = {:home/maria/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alva-Manchego, Scarton, Specia - 2021 - The (Un)Suitability of Automatic Evaluation Metrics for Text Simplification under a Creative Com.pdf:pdf},
journal = {Association for Computational Linguistics},
number = {4},
pages = {862--889},
title = {{The (Un)Suitability of Automatic Evaluation Metrics for Text Simplification under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license}},
url = {https://doi.org/10.1162/COLI},
volume = {47},
year = {2021}
}
